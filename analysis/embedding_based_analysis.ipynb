{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.generate_embeddings import EmbeddingGenerator\n",
    "from vislearnlabpy.embeddings.embedding_store import EmbeddingStore\n",
    "from vislearnlabpy.embeddings.stimuli_loader import ImageExtractor\n",
    "from vislearnlabpy.embeddings.utils import display_search_results, zscore_embeddings, filter_embeddings \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip input transformation\n",
    "transforms_thumbnail = ImageExtractor.get_transformations(\n",
    "        resize_dim=256,\n",
    "        crop_dim=224,\n",
    "        apply_content_crop=True,\n",
    "        apply_center_crop=False,\n",
    "        use_thumbnail=True\n",
    "    )\n",
    "\n",
    "clip_generator = EmbeddingGenerator(model_type=\"clip\", device=\"mps\", output_type=\"doc\", transform=transforms_thumbnail) # change device=\"cpu\" if you are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First getting Beijing and CDM drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "beijing_dir = Path(\"/Volumes/vislearnlab/experiments/drawing/data/beijing/object_drawings\")\n",
    "kisumu_dir = Path(\"/Volumes/vislearnlab/experiments/drawing/data/kisumu/drawings\")\n",
    "categories = [\"airplane\", \"bike\", \"bird\", \"car\", \"cat\", \"chair\", \"cup\", \"hat\", \"house\", \"rabbit\", \"tree\", \"watch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_generator.generate_image_embeddings(output_path=\"beijing_cdm_drawings\", input_dir=beijing_dir, batch_size=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now grabbing Kisumu drawings and getting the text embeddings for each category using the prompt 'drawing of a..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_generator.generate_image_embeddings(output_path=\"kisumu_drawings\", input_dir=kisumu_dir, batch_size=100, overwrite=True)\n",
    "clip_generator.model.text_prompt = \"\"\n",
    "clip_generator.save_text_embeddings([f\"drawing of a {category}\" for category in categories], \"categories\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kisumu_store =  EmbeddingStore.from_doc(\"kisumu_drawings/image_embeddings/clip_image_embeddings_doc\")\n",
    "beijing_cdm_store =  EmbeddingStore.from_doc(\"beijing_cdm_drawings/image_embeddings/clip_image_embeddings_doc\")\n",
    "text_embeddings = EmbeddingStore.from_doc(\"categories/text_embeddings/clip_text_embeddings_doc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choosing embeddings that include verbal cues and not picture cues and separating CDM and Beijing\n",
    "cdm_store = EmbeddingStore()\n",
    "beijing_store = EmbeddingStore()\n",
    "for embedding in beijing_cdm_store.EmbeddingList:\n",
    "    # if this embedding is a verbal cue and not a picture cue\n",
    "    if \"S_\" in embedding.url:\n",
    "        if \"CDM\" in embedding.url:\n",
    "            cdm_store.add_embedding(embedding=embedding.embedding, url=embedding.url)\n",
    "        else:\n",
    "            beijing_store.add_embedding(embedding=embedding.embedding, url=embedding.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Kisumu embeddings: {len(kisumu_store.EmbeddingList)}\")\n",
    "print(f\"Beijing embeddings: {len(beijing_store.EmbeddingList)}\")\n",
    "print(f\"CDM embeddings: {len(cdm_store.EmbeddingList)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract metadata about location, category and age from the file names and calculate probability of the correct category being chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from vislearnlabpy.embeddings.similarity_utils import calculate_probability\n",
    "def extract_beijing_metadata(url):\n",
    "    \"\"\"Extract metadata from Beijing URL format\"\"\"\n",
    "    filename = os.path.basename(url)\n",
    "    # Determine location\n",
    "    location = \"Beijing\" if \"THU\" in filename else \"USA\"\n",
    "    # Extract age\n",
    "    age_match = re.search(r'age(\\d+)', filename)\n",
    "    age = int(age_match.group(1)) if age_match else None\n",
    "    # Extract category (between first and second underscore, after the initial letter)\n",
    "    parts = filename.split('_')\n",
    "    category = parts[1] if len(parts) > 1 else None\n",
    "    # Extract participant ID (everything before .png)\n",
    "    participant_id = filename.replace('.png', '').split(\"_\")[-1]\n",
    "    return {\n",
    "        'location': location,\n",
    "        'age': age,\n",
    "        'category': category,\n",
    "        'participant_id': participant_id\n",
    "    }\n",
    "\n",
    "def extract_kisumu_metadata(url):\n",
    "    \"\"\"Extract metadata from Kisumu URL format\"\"\"\n",
    "    filename = os.path.basename(url)\n",
    "    \n",
    "    # Split on first underscore to get participant and rest\n",
    "    parts = filename.split('_', 1)\n",
    "    participant_id = parts[0]\n",
    "    \n",
    "    # Extract age and add 3\n",
    "    age_match = re.search(r'age(\\d+)', filename)\n",
    "    age = int(age_match.group(1)) + 3 if age_match else None\n",
    "    \n",
    "    # Extract category (between age and trial)\n",
    "    category_match = re.search(r'age\\d+_([^_]+)_trial', filename)\n",
    "    category = category_match.group(1) if category_match else None\n",
    "    category = \"bike\" if category == \"Bicycle\" else category\n",
    "    \n",
    "    return {\n",
    "        'location': 'Kisumu',\n",
    "        'age': age,\n",
    "        'category': category,\n",
    "        'participant_id': participant_id\n",
    "    }\n",
    "\n",
    "def _process_site(\n",
    "    text_embeddings,\n",
    "    docs,\n",
    "    extract_metadata_fn,\n",
    "    recognizability_fn,\n",
    "    results,\n",
    "):\n",
    "    \"\"\"Helper to process embeddings for a single site and extend `results`.\"\"\"\n",
    "    for doc in docs:\n",
    "        md = extract_metadata_fn(doc.url)\n",
    "        if md[\"category\"] and md[\"age\"] is not None:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"location\": md[\"location\"],\n",
    "                    \"recognizability\": recognizability_fn(\n",
    "                        doc.embedding,\n",
    "                        text_embeddings,     \n",
    "                        md[\"category\"].lower()\n",
    "                    ),\n",
    "                    \"age\": md[\"age\"],\n",
    "                    \"participant_id\": md[\"participant_id\"],\n",
    "                    \"drawing_category\": md[\"category\"].lower(),\n",
    "                    \"url\": doc.url,\n",
    "                }\n",
    "            )\n",
    "\n",
    "def process_embeddings(text_embeddings, beijing_list, cdm_list, kisumu_list):\n",
    "    \"\"\"Process all embeddings and create the final dataset.\"\"\"\n",
    "    results = []\n",
    "    # processing beijing\n",
    "    _process_site(\n",
    "        text_embeddings,\n",
    "        beijing_list, \n",
    "        extract_beijing_metadata, \n",
    "        calculate_probability,\n",
    "        results,\n",
    "    )\n",
    "    # processing kisumu\n",
    "    _process_site(\n",
    "        text_embeddings, \n",
    "        kisumu_list,\n",
    "        extract_kisumu_metadata,\n",
    "        calculate_probability,   \n",
    "        results,\n",
    "    )\n",
    "    # processing CDM\n",
    "    _process_site(\n",
    "        text_embeddings, \n",
    "        cdm_list,\n",
    "        extract_beijing_metadata,\n",
    "        calculate_probability,   \n",
    "        results,\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Main processing function\n",
    "def create_drawing_analysis_csv(text_embeddings, beijing_list, cdm_list, kisumu_list, output_file='drawing_analysis.csv'):\n",
    "    \"\"\"\n",
    "    Main function to process all embeddings and create CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process all embeddings\n",
    "    results = process_embeddings(text_embeddings, beijing_list, cdm_list, kisumu_list)\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    print(f\"Total drawings: {len(results)}\")\n",
    "    print(f\"Locations: {df['location'].value_counts().to_dict()}\")\n",
    "    print(f\"Age range: {df['age'].min()} - {df['age'].max()}\")\n",
    "    print(f\"Categories: {df['drawing_category'].value_counts().to_dict()}\")\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizability_df = create_drawing_analysis_csv(text_embeddings.EmbeddingList, beijing_store.EmbeddingList, cdm_store.EmbeddingList, kisumu_store.EmbeddingList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in categories to the embedding stores now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map each URL to the row as a dictionary of fields\n",
    "url_to_fields = recognizability_df.set_index('url').to_dict(orient='index')\n",
    "\n",
    "# Assign fields in loop\n",
    "for store in [beijing_store, kisumu_store, cdm_store]:\n",
    "    for embedding in store.EmbeddingList:\n",
    "        fields = url_to_fields.get(embedding.url, {})\n",
    "        embedding.text = fields.get('drawing_category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating centroid distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from vislearnlabpy.embeddings.similarity_utils import cosine_sim\n",
    "from docarray.utils.filter import filter_docs\n",
    "\n",
    "def calculate_centroid_distances(df, kisumu_store, beijing_store, cdm_store):\n",
    "    \"\"\"\n",
    "    Calculate centroid embeddings for each age-location-category combination and compute distances.\n",
    "    Returns:\n",
    "    df: Original dataframe with added 'distance' column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper function to get embedding list based on location\n",
    "    def get_embedding_list(location):\n",
    "        location_lower = location.lower()\n",
    "        if 'kisumu' in location_lower:\n",
    "            return kisumu_store.EmbeddingList\n",
    "        elif 'beijing' in location_lower:\n",
    "            return beijing_store.EmbeddingList\n",
    "        elif 'usa' in location_lower:\n",
    "            return cdm_store.EmbeddingList\n",
    "        else:\n",
    "            return {}\n",
    "    \n",
    "    # Helper function to extract embedding from store\n",
    "    def get_embedding(url, embedding_list):\n",
    "        matches = filter_docs(embedding_list, {'url': {'$eq': url}})\n",
    "        embedding = matches[0].embedding if matches else None\n",
    "        return embedding\n",
    "    \n",
    "    # Group by age and location to calculate centroids\n",
    "    centroids = {}\n",
    "    \n",
    "    for (age, location, category), group in df.groupby(['age', 'location', 'drawing_category']):\n",
    "        embedding_list = get_embedding_list(location)\n",
    "        embeddings = []\n",
    "        \n",
    "        # Collect all embeddings for this age-location combination\n",
    "        for url in group['url']:\n",
    "            embedding = get_embedding(url, embedding_list)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(np.array(embedding))\n",
    "        \n",
    "        # Calculate centroid if we have embeddings\n",
    "        if embeddings:\n",
    "            centroid = np.mean(embeddings, axis=0)\n",
    "            centroids[(age, location, category)] = centroid\n",
    "        else:\n",
    "            print(f\"Warning: No embeddings found for age={age}, category={category}, location={location}\")\n",
    "            centroids[(age, location, category)] = None\n",
    "    \n",
    "    # Calculate distances for each row in the dataframe\n",
    "    distances = []\n",
    "    distances_euclidean = []\n",
    "    for _, row in df.iterrows():\n",
    "        age = row['age']\n",
    "        location = row['location']\n",
    "        category = row['drawing_category']\n",
    "        url = row['url']\n",
    "        # Get the centroid for this age-location combination\n",
    "        centroid = centroids.get((age, location, category))\n",
    "        if centroid is None:\n",
    "            distances.append(np.nan)\n",
    "            continue\n",
    "        # Get the embedding for this specific URL\n",
    "        embedding_list = get_embedding_list(location)\n",
    "        embedding = get_embedding(url, embedding_list)\n",
    "        \n",
    "        if embedding is None:\n",
    "            distances.append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        embedding = np.stack(embedding)\n",
    "        # Cosine distance calculation\n",
    "        distance = 1 - cosine_sim(embedding, np.stack(centroid))\n",
    "        distance_euclidean = np.linalg.norm(embedding - centroid)\n",
    "        distances_euclidean.append(distance_euclidean)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    df_result = df.copy()\n",
    "    df_result['distance'] = distances\n",
    "    df_result['distance_euclidan'] = distances_euclidean\n",
    "    return df_result, centroids\n",
    "\n",
    "\n",
    "df_with_recognizability_distances, output_centroids = calculate_centroid_distances(recognizability_df, kisumu_store, beijing_store, cdm_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import DocList\n",
    "from vislearnlabpy.embeddings.embedding_store import CLIPImageEmbedding\n",
    "full_embedding_store = EmbeddingStore()\n",
    "full_embedding_store.EmbeddingList = DocList[CLIPImageEmbedding](\n",
    "    cdm_store.EmbeddingList + beijing_store.EmbeddingList + kisumu_store.EmbeddingList\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving all relevant cosine similarity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_sims ends up being a ~400M file here so should think about filtering with text_pairs param or not finding sim values across all drawings\n",
    "image_sims = full_embedding_store.retrieve_similarities()\n",
    "text_sims = text_embeddings.retrieve_similarities(output_path=\"text_sims.csv\")\n",
    "# image to text similarity values\n",
    "cross_sims = full_embedding_store.retrieve_cross_similarity(text_embeddings.EmbeddingList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just storing the file names as the drawing ID for ease of use and understanding and because they are unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sims[['text1', 'text2']] = image_sims[['text1', 'text2']].applymap(lambda p: Path(p).name)\n",
    "cross_sims['text1'] = cross_sims['text1'].apply(lambda p: Path(p).name)\n",
    "df_with_recognizability_distances['id'] = df_with_recognizability_distances['url'].apply(lambda p: Path(p).name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving similarities and recognizability values with new paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sims.to_csv(\"cross_sims.csv\")\n",
    "df_with_recognizability_distances.drop(columns=['url']).to_csv(\"../data/clip_recognizability.csv\")\n",
    "image_sims.to_csv(\"image_sims.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting RDMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from vislearnlabpy.embeddings.similarity_utils import correlate_rdms\n",
    "\n",
    "def plot_heatmaps(matrices, categories, titles, corr_values=None, cmap=\"viridis\", vmin=None, vmax=None, figsize=(18,6), center=None, cbar=True, suptitle=None):\n",
    "    n = len(matrices)\n",
    "    fig, axs = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axs = [axs]\n",
    "    if vmin is None:\n",
    "        vmin = min(m.min() for m in matrices)\n",
    "    if vmax is None:\n",
    "        vmax = max(m.max() for m in matrices)\n",
    "    \n",
    "    for i, (mat, title) in enumerate(zip(matrices, titles)):\n",
    "        sns.heatmap(mat, ax=axs[i], xticklabels=categories, yticklabels=categories,\n",
    "                    cmap=cmap, vmin=vmin, vmax=vmax, center=center, cbar=cbar if i == n-1 else False)\n",
    "        corr_text = f\"\\nMean diagonal: {corr_values[i]:.3f}\" if corr_values else \"\"\n",
    "        axs[i].set_title(title + corr_text)\n",
    "        if \" vs \" in title:\n",
    "            x_label, y_label = title.split(\" vs \")\n",
    "            axs[i].set_xlabel(x_label.strip())\n",
    "            axs[i].set_ylabel(y_label.strip())\n",
    "    if suptitle is not None:\n",
    "        fig.suptitle(suptitle, \n",
    "        fontsize=16\n",
    "        )\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from vislearnlabpy.embeddings.similarity_utils import correlate_rdms\n",
    "\n",
    "# Compute the RDMs \n",
    "cdm_rdm = cdm_store.compute_text_rdm()\n",
    "beijing_rdm = beijing_store.compute_text_rdm()\n",
    "kisumu_rdm = kisumu_store.compute_text_rdm()\n",
    "\n",
    "# change ordering to create clusters: TODO: support custom ordering for our rdm functions\n",
    "rdm_categories = [\"airplane\", \"bike\", \"car\", \"bird\", \"cat\", \"rabbit\", \"tree\", \"house\", \"chair\", \"cup\", \"hat\", \"watch\"]\n",
    "reorder_idx = [categories.index(cat) for cat in rdm_categories]\n",
    "cdm_rdm, beijing_rdm, kisumu_rdm = [\n",
    "    rdm[np.ix_(reorder_idx, reorder_idx)] \n",
    "    for rdm in [cdm_rdm, beijing_rdm, kisumu_rdm]\n",
    "]\n",
    "\n",
    "# Compute correlations\n",
    "corr_beijing_kisumu = correlate_rdms(beijing_rdm, kisumu_rdm)\n",
    "corr_beijing_cdm = correlate_rdms(beijing_rdm, cdm_rdm)\n",
    "corr_kisumu_cdm = correlate_rdms(kisumu_rdm, cdm_rdm)\n",
    "\n",
    "plot_heatmaps(\n",
    "    [beijing_rdm, kisumu_rdm, cdm_rdm],\n",
    "    categories=rdm_categories,\n",
    "    titles=[\"Beijing RDM\", \"Kisumu RDM\", \"CDM RDM\"],\n",
    "    corr_values=None,\n",
    "    cmap=\"viridis\",\n",
    "    figsize=(18,6),\n",
    "    cbar=True,\n",
    "    suptitle= f\"Correlations:\\nBeijing-Kisumu: {corr_beijing_kisumu:.3f} | \"\n",
    "    f\"Beijing-CDM: {corr_beijing_cdm:.3f} | Kisumu-CDM: {corr_kisumu_cdm:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vislearnlabpy.embeddings.similarity_utils import cosine_matrix\n",
    "\n",
    "def get_mean_embeddings(embedding_list, categories):\n",
    "    means = []\n",
    "    for cat in categories:\n",
    "        embeddings = [emb.embedding for emb in embedding_list if emb.text == cat]\n",
    "        if not embeddings:\n",
    "            means.append(np.zeros_like(embedding_list[0].embedding))  # or np.nan\n",
    "        else:\n",
    "            means.append(np.mean(embeddings, axis=0))\n",
    "    return np.stack(means)\n",
    "\n",
    "# Compute mean embeddings per category\n",
    "kisumu_means = get_mean_embeddings(kisumu_store.EmbeddingList, rdm_categories)\n",
    "beijing_means = get_mean_embeddings(beijing_store.EmbeddingList, rdm_categories)\n",
    "cdm_means = get_mean_embeddings(cdm_store.EmbeddingList, rdm_categories)\n",
    "\n",
    "# Compute similarity matrices (cosine similarity)\n",
    "sim_kisumu_beijing = 1 - cosine_matrix(kisumu_means, beijing_means)\n",
    "sim_kisumu_cdm = 1 - cosine_matrix(kisumu_means, cdm_means)\n",
    "sim_cdm_beijing = 1 - cosine_matrix(cdm_means, beijing_means)\n",
    "\n",
    "mean_diags = [np.mean(np.diag(m)) for m in [sim_kisumu_beijing, sim_kisumu_cdm, sim_cdm_beijing]]\n",
    "\n",
    "plot_heatmaps(\n",
    "    [sim_kisumu_beijing, sim_kisumu_cdm, sim_cdm_beijing],\n",
    "    categories=rdm_categories,\n",
    "    titles=[\"Kisumu vs Beijing\", \"Kisumu vs CDM\", \"CDM vs Beijing\"],\n",
    "    corr_values=mean_diags,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    figsize=(20,6)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllpy_draw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
